{"cells":[{"cell_type":"markdown","id":"b65b5991","metadata":{},"source":["# Get data into a PySpark DataFrame"]},{"cell_type":"code","execution_count":1,"id":"e495156e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Url for flight data\n","url = \"gs://my-bigdata-project-mp/landing/itineraries.csv\"\n","\n","# Load data into a PySpark DataFrame\n","clean_df = spark.read.csv(url, header=True, inferSchema=True)"]},{"cell_type":"code","execution_count":2,"id":"3b53d62c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- legId: string (nullable = true)\n"," |-- searchDate: date (nullable = true)\n"," |-- flightDate: date (nullable = true)\n"," |-- startingAirport: string (nullable = true)\n"," |-- destinationAirport: string (nullable = true)\n"," |-- fareBasisCode: string (nullable = true)\n"," |-- travelDuration: string (nullable = true)\n"," |-- elapsedDays: integer (nullable = true)\n"," |-- isBasicEconomy: boolean (nullable = true)\n"," |-- isRefundable: boolean (nullable = true)\n"," |-- isNonStop: boolean (nullable = true)\n"," |-- baseFare: double (nullable = true)\n"," |-- totalFare: double (nullable = true)\n"," |-- seatsRemaining: integer (nullable = true)\n"," |-- totalTravelDistance: integer (nullable = true)\n"," |-- segmentsDepartureTimeEpochSeconds: string (nullable = true)\n"," |-- segmentsDepartureTimeRaw: string (nullable = true)\n"," |-- segmentsArrivalTimeEpochSeconds: string (nullable = true)\n"," |-- segmentsArrivalTimeRaw: string (nullable = true)\n"," |-- segmentsArrivalAirportCode: string (nullable = true)\n"," |-- segmentsDepartureAirportCode: string (nullable = true)\n"," |-- segmentsAirlineName: string (nullable = true)\n"," |-- segmentsAirlineCode: string (nullable = true)\n"," |-- segmentsEquipmentDescription: string (nullable = true)\n"," |-- segmentsDurationInSeconds: string (nullable = true)\n"," |-- segmentsDistance: string (nullable = true)\n"," |-- segmentsCabinCode: string (nullable = true)\n","\n"]}],"source":["# Display our inital Schema before all changes.\n","clean_df.printSchema()"]},{"cell_type":"markdown","id":"346ff8db","metadata":{},"source":["# Remove NULLS\n","\n","#### From the Exploratory Data Analysis, we know that the only columns with NULLS are...\n","- totalTravelDistance\n","- segmentsEquipmentDescription"]},{"cell_type":"code","execution_count":4,"id":"6b6ebeeb","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["82138753"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Count records BEFORE dropping nulls\n","clean_df.count()"]},{"cell_type":"code","execution_count":5,"id":"aa67ee4e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["76044221"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Count records AFTER dropping nulls in totalTravelDistance\n","clean_df = clean_df.filter(\"totalTravelDistance is not NULL\")\n","clean_df.count()"]},{"cell_type":"code","execution_count":6,"id":"ee924396","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["74754290"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Count records AFTER dropping nulls in segmentsEquipmentDescription\n","clean_df = clean_df.filter(\"segmentsEquipmentDescription is not NULL\")\n","clean_df.count()"]},{"cell_type":"markdown","id":"8aa5df27","metadata":{},"source":["# Remove Outliers"]},{"cell_type":"code","execution_count":7,"id":"e0318ac8","metadata":{},"outputs":[],"source":["#Although this function could be part of the feature engineering process, \n","# we do it here to remove outliers before we start feature engineering.\n","\n","\n","# We will extract the hour and minutes from travelDuration and combine them into a new column called travelDurationMinutes\n","\n","#We will use these functions to extract the numbers from the strings in travelDuration\n","from pyspark.sql.functions import regexp_extract, col, when, expr\n","\n","def transform_travel_duration(spark_df):\n","    \n","    # Define regex patterns to capture hours and minutes\n","    hours_pattern = \"PT(\\\\d+)H\"       # Captures the digits before 'H' in the \"PT#H\" format\n","    minutes_pattern = \"H(\\\\d+)M\"       # Captures the digits before 'M' in the \"#M\" format after 'H'\n","    only_minutes_pattern = \"PT(\\\\d+)M\" # For cases with only minutes (e.g., \"PT20M\")\n","\n","    # Extract hours and minutes, converting to integers\n","    df_extracted = spark_df \\\n","        .withColumn(\"hours\", regexp_extract(col(\"travelDuration\"), hours_pattern, 1).cast(\"int\")) \\\n","        .withColumn(\"minutes\", when(col(\"travelDuration\").rlike(only_minutes_pattern),\n","                                     regexp_extract(col(\"travelDuration\"), only_minutes_pattern, 1))\n","                    .otherwise(regexp_extract(col(\"travelDuration\"), minutes_pattern, 1)).cast(\"int\"))\n","\n","    # Calculate total minutes\n","    df_with_total_minutes = df_extracted.withColumn(\n","        \"travelDurationMinutes\",\n","        expr(\"coalesce(hours, 0) * 60 + coalesce(minutes, 0)\")\n","    )\n","\n","    # Get these new columns into our df, then drop the two unnecessary columns\n","    spark_df = df_with_total_minutes\n","\n","    # Compare travelDuration and travelDurationMinutes to make sure the values are correct\n","    spark_df = spark_df.drop(\"hours\",\"minutes\")\n","    \n","    # Finally, drop travelDuration as it is no longer useful\n","    spark_df = spark_df.drop(\"travelDuration\")\n","    \n","    return spark_df"]},{"cell_type":"code","execution_count":8,"id":"16f47198","metadata":{},"outputs":[],"source":["#FUNCTION 1\n","# we will make a function that takes a pyspark df, column name, min, max, as arguments\n","#\n","# it modifies the pyspark dataframe to enforce the min and max values in the given column. \n","#   - specifically, it will then remove any value equal to or above max, and any value equal to or below min\n","\n","\n","def set_min_max_col(spark_df, col_name: str, min: float, max: float):\n","    new_df = spark_df.where((col(col_name) <= max) & (col(col_name) >= min))\n","    return new_df\n","\n","\n","#FUNCTION 2\n","# Uses the previous function multiple times\n","# We come up with min max manually with new found knowledge from the Exploratory Data Analysis script.\n","\n","def trim_outliers(spark_df):\n","    spark_df = set_min_max_col(spark_df, 'elapsedDays', 0, 1.2)\n","    spark_df = set_min_max_col(spark_df, \"baseFare\", 0, 740)\n","    spark_df = set_min_max_col(spark_df, \"totalFare\", 0, 825)\n","    spark_df = set_min_max_col(spark_df, \"travelDurationMinutes\", 0, 1000) # remember to call transform_travel_duration() first.\n","    spark_df = set_min_max_col(spark_df, \"totalTravelDistance\",0,4700)\n","    spark_df = set_min_max_col(spark_df, \"seatsRemaining\",0,20)\n","    return spark_df"]},{"cell_type":"code","execution_count":9,"id":"b720cff6","metadata":{},"outputs":[],"source":["#Turn travel duration into a column we can measure. Also allows us to remove its outliers.\n","clean_df = transform_travel_duration(clean_df)"]},{"cell_type":"code","execution_count":10,"id":"abd2e8b6","metadata":{},"outputs":[],"source":["#Transform all columns with outliers\n","clean_df = trim_outliers(clean_df)"]},{"cell_type":"code","execution_count":11,"id":"95b6472f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["72733945"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["#Count records after forcing a min-max on all numeric columns\n","clean_df.count()"]},{"cell_type":"markdown","id":"452062c3","metadata":{},"source":["# Display our final schema and write to /cleaned in our google bucket"]},{"cell_type":"code","execution_count":12,"id":"d41bd158","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- legId: string (nullable = true)\n"," |-- searchDate: date (nullable = true)\n"," |-- flightDate: date (nullable = true)\n"," |-- startingAirport: string (nullable = true)\n"," |-- destinationAirport: string (nullable = true)\n"," |-- fareBasisCode: string (nullable = true)\n"," |-- elapsedDays: integer (nullable = true)\n"," |-- isBasicEconomy: boolean (nullable = true)\n"," |-- isRefundable: boolean (nullable = true)\n"," |-- isNonStop: boolean (nullable = true)\n"," |-- baseFare: double (nullable = true)\n"," |-- totalFare: double (nullable = true)\n"," |-- seatsRemaining: integer (nullable = true)\n"," |-- totalTravelDistance: integer (nullable = true)\n"," |-- segmentsDepartureTimeEpochSeconds: string (nullable = true)\n"," |-- segmentsDepartureTimeRaw: string (nullable = true)\n"," |-- segmentsArrivalTimeEpochSeconds: string (nullable = true)\n"," |-- segmentsArrivalTimeRaw: string (nullable = true)\n"," |-- segmentsArrivalAirportCode: string (nullable = true)\n"," |-- segmentsDepartureAirportCode: string (nullable = true)\n"," |-- segmentsAirlineName: string (nullable = true)\n"," |-- segmentsAirlineCode: string (nullable = true)\n"," |-- segmentsEquipmentDescription: string (nullable = true)\n"," |-- segmentsDurationInSeconds: string (nullable = true)\n"," |-- segmentsDistance: string (nullable = true)\n"," |-- segmentsCabinCode: string (nullable = true)\n"," |-- travelDurationMinutes: integer (nullable = false)\n","\n"]}],"source":["# Here is the final schema for the clean DataFrame\n","clean_df.printSchema()"]},{"cell_type":"code","execution_count":13,"id":"b938de32","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/11/16 02:07:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","                                                                                \r"]}],"source":["# We will now write this back to /cleaned\n","\n","url = \"gs://my-bigdata-project-mp/cleaned\"\n","\n","clean_df.write.parquet(path=url, mode=\"overwrite\")"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}